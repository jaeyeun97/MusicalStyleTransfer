@INPROCEEDINGS{6854950, 
author={S. Dieleman and B. Schrauwen}, 
booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={End-to-end learning for music audio}, 
year={2014}, 
volume={}, 
number={}, 
pages={6964-6968}, 
abstract={Content-based music information retrieval tasks have traditionally been solved using engineered features and shallow processing architectures. In recent years, there has been increasing interest in using feature learning and deep architectures instead, thus reducing the required engineering effort and the need for prior knowledge. However, this new approach typically still relies on mid-level representations of music audio, e.g. spectrograms, instead of raw audio signals. In this paper, we investigate whether it is possible to apply feature learning directly to raw audio signals. We train convolutional neural networks using both approaches and compare their performance on an automatic tagging task. Although they do not outperform a spectrogram-based approach, the networks are able to autonomously discover frequency decompositions from raw audio, as well as phase-and translation-invariant feature representations.}, 
keywords={content-based retrieval;learning (artificial intelligence);music;end-to-end learning;music audio;content-based music information retrieval tasks;convolutional neural networks training;automatic tagging task;spectrogram-based approach;frequency decompositions;raw audio;phase-and translation-invariant feature representations;Convolution;Spectrogram;Neural networks;Music information retrieval;Speech;Computer architecture;feature learning;end-to-end learning;convolutional neural networks;music information retrieval;automatic tagging}, 
doi={10.1109/ICASSP.2014.6854950}, 
ISSN={1520-6149}, 
month={May},}
